import json
import os
import re

import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer, util

# ================== CONFIGURA√á√ïES ==================

# Caminho do seu dataset JSONL com as explica√ß√µes
# Estrutura esperada de cada linha:
# {
#   "id": "...",
#   "chunk": {"text": "...", ...},
#   "explanations": [
#       {"text": "...", "label": "correct"},
#       {"text": "...", "label": "incomplete"},
#       {"text": "...", "label": "incorrect"}
#   ],
#   ...
# }
JSONL_FILE = "explainrag_hotpot_llama.jsonl"

# Sa√≠das
CSV_OUT = "explanations_sentencewise_embeddings_metrics.csv"
SUMMARY_OUT = "explanations_sentencewise_embeddings_summary_by_label.csv"
PLOT_F1_BOX = "emb_f1_by_label_boxplot.png"
PLOT_PREC_BOX = "emb_precision_by_label_boxplot.png"
PLOT_REC_BOX = "emb_recall_by_label_boxplot.png"

# Modelo de embeddings (bom, pequeno, r√°pido)
EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# Threshold de similaridade para considerar que uma senten√ßa est√° "coberta"
# (0 a 1). 0.7‚Äì0.8 costuma ser um bom ponto de partida.
SIM_THRESHOLD = 0.75

# ====================================================


def split_into_sentences(text: str):
    """
    Split simples em senten√ßas usando pontua√ß√£o (. ! ?).
    N√£o √© perfeito, mas √© suficiente para nosso cen√°rio.
    """
    text = text.strip()
    if not text:
        return []
    sentences = re.split(r'(?<=[\.\!\?])\s+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences


def compute_sentencewise_prf(chunk: str, explanation: str,
                             model: SentenceTransformer,
                             threshold: float = 0.75):
    """
    Calcula precision, recall, f1 com base em cobertura de senten√ßas via embeddings.

    - chunk: texto de origem
    - explanation: explica√ß√£o gerada
    - threshold: similaridade m√≠nima para considerar que h√° cobertura
    """

    chunk_sents = split_into_sentences(chunk)
    expl_sents = split_into_sentences(explanation)

    # Casos borda
    if not chunk_sents or not expl_sents:
        return 0.0, 0.0, 0.0, len(chunk_sents), len(expl_sents)

    # Embeddings por senten√ßa
    emb_chunk = model.encode(chunk_sents, convert_to_tensor=True)
    emb_expl = model.encode(expl_sents, convert_to_tensor=True)

    # Matriz de similaridade (len_chunk x len_expl)
    sim_matrix = util.cos_sim(emb_chunk, emb_expl)  # tensor

    # Cobertura de senten√ßas do chunk:
    # para cada senten√ßa do chunk, verifica se alguma da explica√ß√£o bate >= threshold
    chunk_covered = (sim_matrix >= threshold).any(dim=1).cpu().numpy()

    # Ancoragem de senten√ßas da explica√ß√£o:
    # para cada senten√ßa da explica√ß√£o, verifica se alguma do chunk bate >= threshold
    expl_grounded = (sim_matrix >= threshold).any(dim=0).cpu().numpy()

    num_chunk = len(chunk_sents)
    num_expl = len(expl_sents)

    covered_chunk = chunk_covered.sum()
    grounded_expl = expl_grounded.sum()

    # Precision: fra√ß√£o de senten√ßas da explica√ß√£o que est√£o ancoradas no chunk
    if num_expl == 0:
        precision = 0.0
    else:
        precision = grounded_expl / num_expl

    # Recall: fra√ß√£o de senten√ßas do chunk cobertas pela explica√ß√£o
    if num_chunk == 0:
        recall = 0.0
    else:
        recall = covered_chunk / num_chunk

    # F1
    if precision + recall == 0:
        f1 = 0.0
    else:
        f1 = 2 * precision * recall / (precision + recall)

    return float(precision), float(recall), float(f1), num_chunk, num_expl


def load_explanations(jsonl_path):
    """
    L√™ o JSONL e extrai:
    - id da inst√¢ncia
    - √≠ndice da explica√ß√£o dentro da inst√¢ncia
    - label (correct / incomplete / incorrect)
    - texto da explica√ß√£o
    - chunk_text
    """
    records = []

    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            ex = json.loads(line)
            ex_id = ex.get("id", "")

            chunk_text = ex.get("chunk", {}).get("text", "")
            if not chunk_text:
                continue

            for idx, exp in enumerate(ex.get("explanations", [])):
                label = exp.get("label", "")
                text = exp.get("text", "").strip()
                if not text:
                    continue

                records.append({
                    "example_id": ex_id,
                    "exp_index": idx,
                    "label": label,
                    "chunk_text": chunk_text,
                    "explanation_text": text
                })

    return records


def main():
    if not os.path.exists(JSONL_FILE):
        raise FileNotFoundError(f"Arquivo JSONL n√£o encontrado: {JSONL_FILE}")

    print(f"üì• Lendo dataset de: {JSONL_FILE}")
    records = load_explanations(JSONL_FILE)
    print(f"Total de explica√ß√µes carregadas: {len(records)}")

    if not records:
        print("Nenhuma explica√ß√£o encontrada. Verifique o arquivo JSONL.")
        return

    print(f"üî¢ Carregando modelo de embeddings: {EMBEDDING_MODEL}")
    model = SentenceTransformer(EMBEDDING_MODEL)

    precisions = []
    recalls = []
    f1s = []
    chunk_lens = []
    expl_lens = []

    for rec in tqdm(records, desc="Calculando m√©tricas sentence-wise (embeddings)"):
        p, r, f1, n_chunk, n_expl = compute_sentencewise_prf(
            rec["chunk_text"],
            rec["explanation_text"],
            model,
            threshold=SIM_THRESHOLD
        )
        precisions.append(p)
        recalls.append(r)
        f1s.append(f1)
        chunk_lens.append(n_chunk)
        expl_lens.append(n_expl)

    # DataFrame com resultados
    df = pd.DataFrame(records)
    df["precision"] = precisions
    df["recall"] = recalls
    df["f1"] = f1s
    df["chunk_num_sentences"] = chunk_lens
    df["expl_num_sentences"] = expl_lens

    # Salva CSV detalhado
    df.to_csv(CSV_OUT, index=False)
    print(f"‚úÖ Resultados detalhados salvos em: {CSV_OUT}")

    # Resumo por label
    summary = df.groupby("label")[["precision", "recall", "f1"]].agg(["mean", "std", "count"])
    summary.to_csv(SUMMARY_OUT)
    print(f"‚úÖ Resumo por label salvo em: {SUMMARY_OUT}")
    print("\nüìä Resumo por label (m√©dia / desvio padr√£o / n):")
    print(summary)

    # ============ Gr√°ficos ============

    # F1 por label
    plt.figure(figsize=(6, 4))
    df.boxplot(column="f1", by="label")
    plt.title("Sentence-wise F1 (embeddings) por label de explica√ß√£o")
    plt.suptitle("")  # remove t√≠tulo autom√°tico do pandas
    plt.xlabel("Label")
    plt.ylabel("F1 (cobertura de senten√ßas)")
    plt.tight_layout()
    plt.savefig(PLOT_F1_BOX)
    plt.close()
    print(f"üìà Gr√°fico salvo: {PLOT_F1_BOX}")

    # Precision por label
    plt.figure(figsize=(6, 4))
    df.boxplot(column="precision", by="label")
    plt.title("Sentence-wise Precision (embeddings) por label de explica√ß√£o")
    plt.suptitle("")
    plt.xlabel("Label")
    plt.ylabel("Precision (senten√ßas da explica√ß√£o ancoradas)")
    plt.tight_layout()
    plt.savefig(PLOT_PREC_BOX)
    plt.close()
    print(f"üìà Gr√°fico salvo: {PLOT_PREC_BOX}")

    # Recall por label
    plt.figure(figsize=(6, 4))
    df.boxplot(column="recall", by="label")
    plt.title("Sentence-wise Recall (embeddings) por label de explica√ß√£o")
    plt.suptitle("")
    plt.xlabel("Label")
    plt.ylabel("Recall (senten√ßas do chunk cobertas)")
    plt.tight_layout()
    plt.savefig(PLOT_REC_BOX)
    plt.close()
    print(f"üìà Gr√°fico salvo: {PLOT_REC_BOX}")

    print("\n‚úÖ Avalia√ß√£o conclu√≠da.")


if __name__ == "__main__":
    main()
